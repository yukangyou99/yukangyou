<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Home | Kangyou (Calvin) Yu</title>

  <!--
    - favicon
  -->
  <link rel="shortcut icon" href="./assets/images/logo.ico" type="image/x-icon">

  <!--
    - custom css link
  -->
  <link rel="stylesheet" href="./assets/css/style.css">

  <!--
    - google font link
  -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap" rel="stylesheet">
</head>

<body>

  <!--
    - #MAIN
  -->

  <main>

    <!--
      - #SIDEBAR
    -->

    <aside class="sidebar" data-sidebar>

      <div class="sidebar-info">

        <figure class="avatar-box">
          <img src="./assets/images/my-avatar.png" alt="Richard hanrick" width="80">
        </figure>

        <div class="info-content">
          <h1 class="name" title="Kangyou Yu">Kangyou (Calvin) Yu</h1>

          <p class="title" style="padding-bottom: 5px;padding-top: 5px;border-top-width: 5px;border-bottom-width: 5px;margin-bottom: 2.5px;margin-top: 2.5px;">CS PhD in UCSB</p>
          <p class="title" style="padding-bottom: 5px;padding-top: 5px;border-top-width: 5px;border-bottom-width: 5px;margin-bottom: 2.5px;margin-top: 2.5px;">HCI, AR/VR/XR</p>
        </div>

        <button class="info_more-btn" data-sidebar-btn>
          <span>Show Contacts</span>

          <ion-icon name="chevron-down"></ion-icon>
        </button>

      </div>

      <div class="sidebar-info_more">

        <div class="separator"></div>

        <ul class="contacts-list">

          <li class="contact-item">

            <div class="icon-box">
              <ion-icon name="mail-outline"></ion-icon>
            </div>

            <div class="contact-info">
              <p class="contact-title">Email</p>

              <a href="mailto:kangyouyu@ucsb.edu" class="contact-link">kangyouyu [at] ucsb [dot] edu</a>
            </div>

          </li>

          <li class="contact-item">

            <div class="icon-box">
              <ion-icon name="phone-portrait-outline"></ion-icon>
            </div>

            <div class="contact-info">
              <p class="contact-title">Phone</p>

              <a href="tel:+18056895662" class="contact-link">+1 (805) 689-5662</a>
            </div>

          </li>

          

          <li class="contact-item">

            <div class="icon-box">
              <ion-icon name="location-outline"></ion-icon>
            </div>

            <div class="contact-info">
              <p class="contact-title">Location</p>

              <address>Santa Barbara, California, US</address>
            </div>

          </li>

        </ul>

        <div class="separator"></div>

        <ul class="social-list">

          <li class="social-item">
            <a href="https://www.linkedin.com/in/kangyou-yu-59b6371a4/" class="social-link">
              <ion-icon name="logo-linkedin"></ion-icon>
            </a>
          </li>

          <li class="social-item">
            <a href="https://twitter.com/yukangyou99" class="social-link">
              <ion-icon name="logo-twitter"></ion-icon>
            </a>
          </li>
          
          <li class="social-item">
            <a href="https://scholar.google.com/citations?user=x1V3pcUAAAAJ&hl=en" class="social-link">
              <ion-icon name="logo-google scholar"></ion-icon>
            </a>
          </li>

 

        </ul>

      </div>

    </aside>





    <!--
      - #main-content
    -->

    <div class="main-content">

      <!--
        - #NAVBAR
      -->

      <nav class="navbar">

        <ul class="navbar-list">

          <li class="navbar-item">
            <button class="navbar-link  active" data-nav-link>Bio</button>
          </li>

          <li class="navbar-item">
            <button class="navbar-link" data-nav-link>Resume</button>
          </li>

          <li class="navbar-item">
            <button class="navbar-link" data-nav-link>Projects</button>
          </li>

          <li class="navbar-item">
            <button class="navbar-link" data-nav-link>Download CV</button>
          </li>
          
        </ul>

      </nav>





      <!--
        - #ABOUT
      -->

      <article class="about  active" data-page="about">

        <header>
          <h2 class="h2 article-title">About me</h2>
        </header>

        <section class="about-text">
          <p>
            I'm Kangyou Yu (Calvin), a first-year CS PhD in <a target="_blank" href="http://ilab.cs.ucsb.edu/">FourEyesLab</a> 
            (directed by <a target="_blank" href="https://sites.cs.ucsb.edu/~holl/">Prof. Tobias Höllerer</a>) 
            at University of California, Santa Barbara (UCSB). 
            
            Prior to joining UCSB, I received my BSc degree in Information and Computing Science from <a target="_blank" href="https://www.xjtlu.edu.cn/en/">Xi’an Jiaotong-Liverpool University</a> (Suzhou, China) 
            
            and University of Liverpool (Liverpool, United Kingdom) in 2018 and 
            
            was an undergraduate research assistant at X-CHI Lab, supervised by <a target="_blank"  href="https://www.xjtlu.edu.cn/en/departments/academic-departments/school-of-advanced-technology/staff/haining-liang">Prof. Hai-Ning Liang</a> 
            
            and <a target="_blank" href="https://xuwenge.github.io/">Dr. Wenge Xu</a> (now are a Lecturer at Birmingham City University).
          </p>
          
          <p>
            I also worked as a remote research intern in the VVISE Lab, supervised by Prof. Wolfgang Stuerzlinger (Simon Fraser University, Canada) and Prof. Anil Ufuk Batmaz (now at Kadir Has University, Turkey). 
          </p>

          <p>
           My research interest lies on Human-Computer Interaction (HCI) and focus on understanding human daily behaviors, involving full body into interaction process, and providing more feedback (Haptics, Auditory...) to design more natural interactive methods/techniques in AR/VR. 
            
           I am also interested in games technologies, visualization and exergames.
          </p>
        </section>
        
        
        <!--
        - #education
      -->

        <section class="timeline">

          <div class="title-wrapper">
            <div class="icon-box">
              <ion-icon name="book-outline"></ion-icon>
            </div>

            <h3 class="h3">Education</h3>
          </div>

          <ol class="timeline-list">

            <li class="timeline-item">

              <h4 class="h4 timeline-item-title">University of California, Santa Barbara (UCSB)</h4>
              
              <h6 class="h6 timeline-item-title">Ph.D. in CS </h6>

              <span>2022.09 — Present</span> 


            </li>
            
            <li class="timeline-item">

              <h4 class="h4 timeline-item-title">University of Liverpool (UoL)</h4>
              
              <h6 class="h6 timeline-item-title">BS.c. (with honors) in CS </h6>

              <span>2018.09-2022.07</span>

            </li>

            <li class="timeline-item">

              <h4 class="h4 timeline-item-title"> Xi'an Jiaotong-Liverpool University (XJTLU)</h4>
              
              <h6 class="h6 timeline-item-title">BS.c. (with honors) in CS </h6>

              <span>2018.09-2022.07</span>

            </li>

          </ol>

        </section>

        <section class="timeline">

          <div class="title-wrapper">
            <div class="icon-box">
              <ion-icon name="book-outline"></ion-icon>
            </div>

            <h3 class="h3">Research Experience</h3>
          </div>

          <ol class="timeline-list">

            <li class="timeline-item">

              <h4 class="h4 timeline-item-title">FourEyesLab, University of California, Santa Barbara</h4>

              <h6 class="h6 timeline-item-title">Graduate Research Assistant</h6>
              
              <span>2022.09 — Present</span>

            </li>

            <li class="timeline-item">

              <h4 class="h4 timeline-item-title">APEX Lab, Hongkong University of Science and Technology, Guangzhou</h4>

              <h6 class="h6 timeline-item-title">Remote Research Intern</h6>
              
              <span>2022.06 - 2022.09</span>
              
            </li>

            <li class="timeline-item">

              <h4 class="h4 timeline-item-title">VVISE Lab, Simon Fraser University</h4>
              
              <h6 class="h6 timeline-item-title">Remote Research Intern</h6>

              <span>2021.04 - 2022.10</span>
   
            </li>
            
             <li class="timeline-item">

              <h4 class="h4 timeline-item-title">X-CHI Lab, Xi'an Jiaotong-Liverpool University</h4>
               
              <h6 class="h6 timeline-item-title">Undergraduate Research Assistant</h6>

              <span>2019.05 - 2022.10</span>
   
            </li>

          </ol>

        </section>

      


        <!--
          - selected projects
        -->

<!--         <section class="service">

          <h3 class="h3 service-title">Research Interests</h3>

          <ul class="service-list">

            <li class="service-item">

              <div class="service-icon-box">
                <img src="./assets/images/icon-design.svg
                " alt="design icon" width="40">
              </div>

              <div class="service-content-box">
                
             
                <h4 class="h4 service-item-title">Virtual Reality, Augmented Reality and Mixed Reality</h4>

                
              </div>

            </li>

            <li class="service-item">

              <div class="service-icon-box">
                <img src="./assets/images/icon-dev.svg" alt="Web development icon" width="40">
              </div>

              <div class="service-content-box">
                <h4 class="h4 service-item-title">Web development</h4>

                <p class="service-item-text">
                  High-quality development of sites at the professional level.
                </p>
              </div>

            </li>

            <li class="service-item">

              <div class="service-icon-box">
                <img src="./assets/images/icon-app.svg" alt="mobile app icon" width="40">
              </div>

              <div class="service-content-box">
                <h4 class="h4 service-item-title">Mobile apps</h4>

                <p class="service-item-text">
                  Professional development of applications for iOS and Android.
                </p>
              </div>

            </li>

            <li class="service-item">

              <div class="service-icon-box">
                <img src="./assets/images/icon-photo.svg" alt="camera icon" width="40">
              </div>

              <div class="service-content-box">
                <h4 class="h4 service-item-title">Photography</h4>

                <p class="service-item-text">
                  I make high-quality photos of any category at a professional level.
                </p>
              </div>

            </li>

          </ul>

        </section>
 -->

        <!--
          - testimonials
        -->

        <section class="testimonials">

          <h3 class="h3 testimonials-title">News</h3>

          <ul class="testimonials-list has-scrollbar">

            <li class="testimonials-item">
              <div class="content-card" data-testimonials-item>

<!--                 <figure class="testimonials-avatar-box">
                  <img src="./assets/images/avatar-1.png" alt="Daniel lewis" width="60" data-testimonials-avatar>
                </figure> -->

                <h4 class="h4 testimonials-item-title" data-testimonials-title>October, 2022</h4>
                

                <div class="testimonials-text" data-testimonials-text>
                  <p>
                    Our paper "Evaluation of Text Selection Techniques in Virtual Reality Head-Mounted Displays" was presented by Wenge Xu online at the IEEE ISMAR 2022, thanks my coauthors. lol
                  </p>
                </div>

              </div>
            </li>

            <li class="testimonials-item">
              <div class="content-card" data-testimonials-item>

<!--                 <figure class="testimonials-avatar-box">
                  <img src="./assets/images/avatar-2.png" alt="Jessica miller" width="60" data-testimonials-avatar>
                </figure> -->

                <h4 class="h4 testimonials-item-title" data-testimonials-title>September, 2022</h4>

                <div class="testimonials-text" data-testimonials-text>
                  <p>
                    I arrived at the Santa Barbara, CA, US, I will start my PhD journey here!
                  </p>
                </div>

              </div>
            </li>

            <li class="testimonials-item">
              <div class="content-card" data-testimonials-item>

<!--                 <figure class="testimonials-avatar-box">
                  <img src="./assets/images/avatar-3.png" alt="Emily evans" width="60" data-testimonials-avatar>
                </figure> -->

                <h4 class="h4 testimonials-item-title" data-testimonials-title>September, 2022</h4>

                <div class="testimonials-text" data-testimonials-text>
                  <p>
                    Submitted a full paper to CHI 2023, thank Prof. Mingming Fan for giving me such a great internship chance. 🤞
                  </p>
                </div>

              </div>
            </li>

            <li class="testimonials-item">
              <div class="content-card" data-testimonials-item>

<!--                 <figure class="testimonials-avatar-box">
                  <img src="./assets/images/avatar-4.png" alt="Henry william" width="60" data-testimonials-avatar>
                </figure> -->

                <h4 class="h4 testimonials-item-title" data-testimonials-title>August, 2022</h4>

                <div class="testimonials-text" data-testimonials-text>
                  <p>
                   A co-authored paper was accepted by the ISMAR 2022, congratulations to Wenge!
                  </p>
                </div>

              </div>
            </li>

          </ul>

        </section>


        <!--
          - testimonials modal
        -->

        <div class="modal-container" data-modal-container>

          <div class="overlay" data-overlay></div>

          <section class="testimonials-modal">

            <button class="modal-close-btn" data-modal-close-btn>
              <ion-icon name="close-outline"></ion-icon>
            </button>

            <div class="modal-img-wrapper">
<!--               <figure class="modal-avatar-box">
                <img src="./assets/images/avatar-1.png" alt="Daniel lewis" width="80" data-modal-img>
              </figure>
 -->
              <img src="./assets/images/icon-quote.svg" alt="quote icon">
            </div>

            <div class="modal-content">

              <h4 class="h3 modal-title" data-modal-title>Daniel lewis</h4>

              <time datetime="2021-06-14" data-modal-datetime>13 September, 2021</time>

              <div data-modal-text>
                <p>
                  Richard was hired to create a corporate identity. We were very pleased with the work done. She has a
                  lot of experience
                  and is very concerned about the needs of client. Lorem ipsum dolor sit amet, ullamcous cididt
                  consectetur adipiscing
                  elit, seds do et eiusmod tempor incididunt ut laborels dolore magnarels alia.
                </p>
              </div>

            </div>

          </section>

        </div>
        
        <div class="modal-container" data-modal-container>

          <div class="overlay" data-overlay></div>

          <section class="testimonials-modal">

            <button class="modal-close-btn" data-modal-close-btn>
              <ion-icon name="close-outline"></ion-icon>
            </button>

            <div class="modal-img-wrapper">
              <figure class="modal-avatar-box">
                <img src="./assets/images/avatar-1.png" alt="Daniel lewis" width="80" data-modal-img>
              </figure>

              <img src="./assets/images/icon-quote.svg" alt="quote icon">
            </div>

            <div class="modal-content">

              <h4 class="h3 modal-title" data-modal-title>Daniel lewis</h4>

              <time datetime="2021-06-14">15 September, 2021</time>

              <div data-modal-text>
                <p>
                  Richard was hired to create a corporate identity. We were very pleased with the work done. She has a
                  lot of experience
                  and is very concerned about the needs of client. Lorem ipsum dolor sit amet, ullamcous cididt
                  consectetur adipiscing
                  elit, seds do et eiusmod tempor incididunt ut laborels dolore magnarels alia.
                </p>
              </div>

            </div>

          </section>

        </div>


        <!--
          - clients
        -->

        <section class="clients">

          <h3 class="h3 clients-title">Clients</h3>

          <ul class="clients-list has-scrollbar">

            <li class="clients-item">
              <a href="#">
                <img src="./assets/images/logo-1-color.png" alt="client logo">
              </a>
            </li>

            <li class="clients-item">
              <a href="#">
                <img src="./assets/images/logo-2-color.png" alt="client logo">
              </a>
            </li>

            <li class="clients-item">
              <a href="#">
                <img src="./assets/images/logo-3-color.png" alt="client logo">
              </a>
            </li>

            <li class="clients-item">
              <a href="#">
                <img src="./assets/images/logo-4-color.png" alt="client logo">
              </a>
            </li>

            <li class="clients-item">
              <a href="#">
                <img src="./assets/images/logo-5-color.png" alt="client logo">
              </a>
            </li>

            <li class="clients-item">
              <a href="#">
                <img src="./assets/images/logo-6-color.png" alt="client logo">
              </a>
            </li>

          </ul>

        </section>

      </article>


<!--        Publications   -->

      
       <article class="resume" data-page="resume">

        <header>
          <h2 class="h2 article-title">Publications</h2>
        </header>

        <section class="skill">

          <h3 class="h3 skills-title">2022</h3>
          <div class="separator"></div>
          
          <h2 id="y2021" class="year">2021</h2>

<div class="publication" data-pub="{&quot;content&quot;:&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;CollabAlly: Accessible Collaboration Awareness in Document Editing | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang*\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang*\&quot;},\&quot;description\&quot;:\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot;,\&quot;headline\&quot;:\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-collabally&quot;,&quot;previous&quot;:{&quot;content&quot;:&quot;While consumer adoption of smart home devices continues\nto grow, privacy concerns reportedly remain a roadblock to\nmass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’\npurchase decisions, and whether such considerations are held\nonly by certain consumer groups but not others. In order to\nunpack the decision-making process of smart home device\nadoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our\nanalysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that\nconsumers can be segmented based on their considerations\ninto three clusters: affordability-oriented, privacy-oriented,\nand reliability-oriented. We present an in-depth quantification\nof consumer considerations on smart home device adoption\nalong with desired privacy and security features consumers\nwish to use to protect their privacy in the smart home.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Natã Barbosa\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;While consumer adoption of smart home devices continues to grow, privacy concerns reportedly remain a roadblock to mass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’ purchase decisions, and whether such considerations are held only by certain consumer groups but not others. In order to unpack the decision-making process of smart home device adoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our analysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that consumers can be segmented based on their considerations into three clusters: affordability-oriented, privacy-oriented, and reliability-oriented. We present an in-depth quantification of consumer considerations on smart home device adoption along with desired privacy and security features consumers wish to use to protect their privacy in the smart home.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;While consumer adoption of smart home devices continues to grow, privacy concerns reportedly remain a roadblock to mass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’ purchase decisions, and whether such considerations are held only by certain consumer groups but not others. In order to unpack the decision-making process of smart home device adoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our analysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that consumers can be segmented based on their considerations into three clusters: affordability-oriented, privacy-oriented, and reliability-oriented. We present an in-depth quantification of consumer considerations on smart home device adoption along with desired privacy and security features consumers wish to use to protect their privacy in the smart home.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2020-smarthomeprivacy\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2020-smarthomeprivacy\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Natã Barbosa\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Natã Barbosa\&quot;},\&quot;description\&quot;:\&quot;While consumer adoption of smart home devices continues to grow, privacy concerns reportedly remain a roadblock to mass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’ purchase decisions, and whether such considerations are held only by certain consumer groups but not others. In order to unpack the decision-making process of smart home device adoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our analysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that consumers can be segmented based on their considerations into three clusters: affordability-oriented, privacy-oriented, and reliability-oriented. We present an in-depth quantification of consumer considerations on smart home device adoption along with desired privacy and security features consumers wish to use to protect their privacy in the smart home.\&quot;,\&quot;headline\&quot;:\&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2020-smarthomeprivacy\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  While consumer adoption of smart home devices continues\nto grow, privacy concerns reportedly remain a roadblock to\nmass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’\npurchase decisions, and whether such considerations are held\nonly by certain consumer groups but not others. In order to\nunpack the decision-making process of smart home device\nadoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our\nanalysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that\nconsumers can be segmented based on their considerations\ninto three clusters: affordability-oriented, privacy-oriented,\nand reliability-oriented. We present an in-depth quantification\nof consumer considerations on smart home device adoption\nalong with desired privacy and security features consumers\nwish to use to protect their privacy in the smart home.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;previous&quot;:{&quot;content&quot;:&quot;An interactive 3D printed model (I3M), model that provides\nan audio description when a user touches components of\nthe model, can help people with visual impairments learn\na variety of concepts. However, it is unknown how an I3M\nshould be designed to serve as an efective teaching aid. To\naddress this gap, we conducted two studies with Teachers\nof the Visually Impaired (TVIs). First, we led two design\nworkshops with 35 TVIs, who modifed sample models and\nadded interactive elements to them. Second, we worked with\nthree TVIs to design three I3Ms in an iterative instructional\ndesign process. At the end of this process, the TVIs used the\nI3Ms we designed to teach their students. We conclude that\nI3Ms should (1) have efective tactile features (e.g., distinctive\npatterns between components), (2) contain both auditory and\nvisual content (e.g., explanatory animations), and (3) consider\npedagogical methods (e.g., overview before details).&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;Designing interactive 3D printed models with Teachers of the Visually Impaired | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Designing interactive 3D printed models with Teachers of the Visually Impaired\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Lei Shi\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;An interactive 3D printed model (I3M), model that provides an audio description when a user touches components of the model, can help people with visual impairments learn a variety of concepts. However, it is unknown how an I3M should be designed to serve as an efective teaching aid. To address this gap, we conducted two studies with Teachers of the Visually Impaired (TVIs). First, we led two design workshops with 35 TVIs, who modifed sample models and added interactive elements to them. Second, we worked with three TVIs to design three I3Ms in an iterative instructional design process. At the end of this process, the TVIs used the I3Ms we designed to teach their students. We conclude that I3Ms should (1) have efective tactile features (e.g., distinctive patterns between components), (2) contain both auditory and visual content (e.g., explanatory animations), and (3) consider pedagogical methods (e.g., overview before details).\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;An interactive 3D printed model (I3M), model that provides an audio description when a user touches components of the model, can help people with visual impairments learn a variety of concepts. However, it is unknown how an I3M should be designed to serve as an efective teaching aid. To address this gap, we conducted two studies with Teachers of the Visually Impaired (TVIs). First, we led two design workshops with 35 TVIs, who modifed sample models and added interactive elements to them. Second, we worked with three TVIs to design three I3Ms in an iterative instructional design process. At the end of this process, the TVIs used the I3Ms we designed to teach their students. We conclude that I3Ms should (1) have efective tactile features (e.g., distinctive patterns between components), (2) contain both auditory and visual content (e.g., explanatory animations), and (3) consider pedagogical methods (e.g., overview before details).\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2019-talkit\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2019-talkit\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Designing interactive 3D printed models with Teachers of the Visually Impaired\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Lei Shi\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Lei Shi\&quot;},\&quot;description\&quot;:\&quot;An interactive 3D printed model (I3M), model that provides an audio description when a user touches components of the model, can help people with visual impairments learn a variety of concepts. However, it is unknown how an I3M should be designed to serve as an efective teaching aid. To address this gap, we conducted two studies with Teachers of the Visually Impaired (TVIs). First, we led two design workshops with 35 TVIs, who modifed sample models and added interactive elements to them. Second, we worked with three TVIs to design three I3Ms in an iterative instructional design process. At the end of this process, the TVIs used the I3Ms we designed to teach their students. We conclude that I3Ms should (1) have efective tactile features (e.g., distinctive patterns between components), (2) contain both auditory and visual content (e.g., explanatory animations), and (3) consider pedagogical methods (e.g., overview before details).\&quot;,\&quot;headline\&quot;:\&quot;Designing interactive 3D printed models with Teachers of the Visually Impaired\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2019-talkit\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  An interactive 3D printed model (I3M), model that provides\nan audio description when a user touches components of\nthe model, can help people with visual impairments learn\na variety of concepts. However, it is unknown how an I3M\nshould be designed to serve as an efective teaching aid. To\naddress this gap, we conducted two studies with Teachers\nof the Visually Impaired (TVIs). First, we led two design\nworkshops with 35 TVIs, who modifed sample models and\nadded interactive elements to them. Second, we worked with\nthree TVIs to design three I3Ms in an iterative instructional\ndesign process. At the end of this process, the TVIs used the\nI3Ms we designed to teach their students. We conclude that\nI3Ms should (1) have efective tactile features (e.g., distinctive\npatterns between components), (2) contain both auditory and\nvisual content (e.g., explanatory animations), and (3) consider\npedagogical methods (e.g., overview before details).\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2019-talkit&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2019-gpk&quot;,&quot;url&quot;:&quot;/publications/2019-gpk&quot;,&quot;relative_path&quot;:&quot;_publications/2019-gpk.html&quot;,&quot;path&quot;:&quot;_publications/2019-gpk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;title&quot;:&quot;GPK: An Efficient Special Character Input Method for Keyboards Using Glide&quot;,&quot;doi&quot;:&quot;10.1145/3290607.3308457&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Xiyuan He*&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;awards&quot;:[&quot;Student Research Competition Winner&quot;],&quot;venue&quot;:&quot;CHI EA&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290607.3308457&quot;,&quot;poster&quot;:&quot;https://drive.google.com/file/d/1f2C561diu7y9jLE7T8vewf476O4lapHs/view?usp=sharing&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2019-gpk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2019-talkit&quot;,&quot;relative_path&quot;:&quot;_publications/2019-talkit.html&quot;,&quot;excerpt&quot;:&quot;An interactive 3D printed model (I3M), model that provides\nan audio description when a user touches components of\nthe model, can help people with visual impairments learn\na variety of concepts. However, it is unknown how an I3M\nshould be designed to serve as an efective teaching aid. To\naddress this gap, we conducted two studies with Teachers\nof the Visually Impaired (TVIs). First, we led two design\nworkshops with 35 TVIs, who modifed sample models and\nadded interactive elements to them. Second, we worked with\nthree TVIs to design three I3Ms in an iterative instructional\ndesign process. At the end of this process, the TVIs used the\nI3Ms we designed to teach their students. We conclude that\nI3Ms should (1) have efective tactile features (e.g., distinctive\npatterns between components), (2) contain both auditory and\nvisual content (e.g., explanatory animations), and (3) consider\npedagogical methods (e.g., overview before details).&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;url&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;relative_path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;title&quot;:&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption&quot;,&quot;authors&quot;:[&quot;Natã Barbosa&quot;,&quot;Zhuohao Zhang&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2020-barbosa.pdf&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=ORc2MwR1uQY&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2020-paper162-slides-barbosa.pdf&quot;,&quot;slug&quot;:&quot;2020-smarthomeprivacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2019-talkit.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;title&quot;:&quot;Designing interactive 3D printed models with Teachers of the Visually Impaired&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300427&quot;,&quot;authors&quot;:[&quot;Lei Shi&quot;,&quot;Holly Lawson&quot;,&quot;Zhuohao Zhang&quot;,&quot;Shiri Azenkot&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300427&quot;,&quot;video&quot;:&quot;https://drive.google.com/file/d/1QuJlDkc0Eq9pVeyYWjHgX7hrk9ksX7Eb/preview&quot;,&quot;blog&quot;:&quot;https://equalentry.com/interactive-3d-printed-models-for-students-with-visual-impairments-accessibility-nyc-meetup-recap/&quot;,&quot;slug&quot;:&quot;2019-talkit&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;relative_path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;excerpt&quot;:&quot;While consumer adoption of smart home devices continues\nto grow, privacy concerns reportedly remain a roadblock to\nmass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’\npurchase decisions, and whether such considerations are held\nonly by certain consumer groups but not others. In order to\nunpack the decision-making process of smart home device\nadoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our\nanalysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that\nconsumers can be segmented based on their considerations\ninto three clusters: affordability-oriented, privacy-oriented,\nand reliability-oriented. We present an in-depth quantification\nof consumer considerations on smart home device adoption\nalong with desired privacy and security features consumers\nwish to use to protect their privacy in the smart home.&quot;,&quot;next&quot;:{&quot;content&quot;:&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;CollabAlly: Accessible Collaboration Awareness in Document Editing | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang*\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang*\&quot;},\&quot;description\&quot;:\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot;,\&quot;headline\&quot;:\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-collabally&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;url&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;relative_path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;title&quot;:&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption&quot;,&quot;authors&quot;:[&quot;Natã Barbosa&quot;,&quot;Zhuohao Zhang&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2020-barbosa.pdf&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=ORc2MwR1uQY&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2020-paper162-slides-barbosa.pdf&quot;,&quot;slug&quot;:&quot;2020-smarthomeprivacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2021-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;excerpt&quot;:&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-webally&quot;,&quot;url&quot;:&quot;/publications/2021-webally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang&quot;,&quot;Zhilin Zhang&quot;,&quot;Haolin Yuan&quot;,&quot;Natã Barbosa&quot;,&quot;Sauvik Das&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2021-zhang-zhuohao.pdf&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2021_slides_zhang_zhuohao.pdf&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=RMqrhzlc2us&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-webally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3441852.3476562&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;ASSETS Demo&quot;,&quot;venue_tags&quot;:[&quot;ASSETS&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3441852.3476562&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=hYvAZofLSjU&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;title&quot;:&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption&quot;,&quot;authors&quot;:[&quot;Natã Barbosa&quot;,&quot;Zhuohao Zhang&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2020-barbosa.pdf&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=ORc2MwR1uQY&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2020-paper162-slides-barbosa.pdf&quot;,&quot;slug&quot;:&quot;2020-smarthomeprivacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2021-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;excerpt&quot;:&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers&quot;,&quot;next&quot;:{&quot;content&quot;:&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang\&quot;},\&quot;description\&quot;:\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot;,\&quot;headline\&quot;:\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-webally&quot;,&quot;previous&quot;:{&quot;content&quot;:&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;CollabAlly: Accessible Collaboration Awareness in Document Editing | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang*\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang*\&quot;},\&quot;description\&quot;:\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot;,\&quot;headline\&quot;:\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-collabally&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;url&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;relative_path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;title&quot;:&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption&quot;,&quot;authors&quot;:[&quot;Natã Barbosa&quot;,&quot;Zhuohao Zhang&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2020-barbosa.pdf&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=ORc2MwR1uQY&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2020-paper162-slides-barbosa.pdf&quot;,&quot;slug&quot;:&quot;2020-smarthomeprivacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2021-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;excerpt&quot;:&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-webally&quot;,&quot;url&quot;:&quot;/publications/2021-webally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang&quot;,&quot;Zhilin Zhang&quot;,&quot;Haolin Yuan&quot;,&quot;Natã Barbosa&quot;,&quot;Sauvik Das&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2021-zhang-zhuohao.pdf&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2021_slides_zhang_zhuohao.pdf&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=RMqrhzlc2us&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-webally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3441852.3476562&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;ASSETS Demo&quot;,&quot;venue_tags&quot;:[&quot;ASSETS&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3441852.3476562&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=hYvAZofLSjU&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2021-webally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;excerpt&quot;:&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.&quot;,&quot;next&quot;:{&quot;content&quot;:&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations. &quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;CollabAlly: Accessible Collaboration Awareness in Document Editing | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2022-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2022-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang*\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang*\&quot;},\&quot;description\&quot;:\&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations.\&quot;,\&quot;headline\&quot;:\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2022-collabally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations. \n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2022-collabally&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-webally&quot;,&quot;url&quot;:&quot;/publications/2021-webally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang&quot;,&quot;Zhilin Zhang&quot;,&quot;Haolin Yuan&quot;,&quot;Natã Barbosa&quot;,&quot;Sauvik Das&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2021-zhang-zhuohao.pdf&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2021_slides_zhang_zhuohao.pdf&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=RMqrhzlc2us&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-webally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2022-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2022-collabally.html&quot;,&quot;excerpt&quot;:&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations. &quot;,&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2022-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3491102.3517635&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;awards&quot;:[&quot;Best Paper Honorable Mentions&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;pdf&quot;:&quot;https://guoanhong.com/papers/CHI22-CollabAlly.pdf&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=s5bcZQlmbzE&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=WK_8cPMVtuA&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2022-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang&quot;,&quot;Zhilin Zhang&quot;,&quot;Haolin Yuan&quot;,&quot;Natã Barbosa&quot;,&quot;Sauvik Das&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2021-zhang-zhuohao.pdf&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2021_slides_zhang_zhuohao.pdf&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=RMqrhzlc2us&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-webally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3441852.3476562&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;ASSETS Demo&quot;,&quot;venue_tags&quot;:[&quot;ASSETS&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3441852.3476562&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=hYvAZofLSjU&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]}">
  <h3 id="/publications/2021-collabally">CollabAlly: Accessible Collaboration Awareness in Document Editing</h3>
  <div class="authors">
    Zhuohao Zhang*, 
    Cheuk Yin Phipson Lee*, 
    Jaylin Herskovitz, 
    JooYoung Seo, 
    Anhong Guo
    
  </div>
  
    <div class="venue">
      
      ASSETS Demo<!--
      
      -->
      2021
    </div>
  

  

  

  
    <div class="extra-links">
    
      <a href="https://dl.acm.org/doi/10.1145/3441852.3476562" target="_blank">
        <i class="far fa-file-pdf" aria-hidden="true"></i> PDF
      </a>
    
    
    
    
    
      <a href="https://www.youtube.com/watch?v=hYvAZofLSjU" target="_blank">
        <i class="fas fa-film" aria-hidden="true"></i> Video
      </a>
    
    
    
    
      <a href="https://www.doi2bib.org/bib/10.1145/3441852.3476562" target="_blank">
        <i class="fas fa-book" aria-hidden="true"></i> Bibtex
      </a>
    
    
    
    
    
    
    </div>
  
</div>

<div class="publication" data-pub="{&quot;content&quot;:&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang\&quot;},\&quot;description\&quot;:\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot;,\&quot;headline\&quot;:\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-webally&quot;,&quot;previous&quot;:{&quot;content&quot;:&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;CollabAlly: Accessible Collaboration Awareness in Document Editing | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang*\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang*\&quot;},\&quot;description\&quot;:\&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some accessibility features, it is still challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who edited or commented where and what in the document). To address this gap, we present CollabAlly, a browser extension that makes extractable collaborative and contextual information in document editing accessible for blind users. With CollabAlly, blind users can easily access collaborators’ information, track real-time or asynchronous content and comment changes, and navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and spatial audio to enhance users’ collaboration awareness in shared documents. Through a series of pilot studies with a coauthor who is blind, CollabAlly’s design was refined to include more information and to be more compatible with existing screen readers\&quot;,\&quot;headline\&quot;:\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2021-collabally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-collabally&quot;,&quot;previous&quot;:{&quot;content&quot;:&quot;While consumer adoption of smart home devices continues\nto grow, privacy concerns reportedly remain a roadblock to\nmass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’\npurchase decisions, and whether such considerations are held\nonly by certain consumer groups but not others. In order to\nunpack the decision-making process of smart home device\nadoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our\nanalysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that\nconsumers can be segmented based on their considerations\ninto three clusters: affordability-oriented, privacy-oriented,\nand reliability-oriented. We present an in-depth quantification\nof consumer considerations on smart home device adoption\nalong with desired privacy and security features consumers\nwish to use to protect their privacy in the smart home.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Natã Barbosa\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;While consumer adoption of smart home devices continues to grow, privacy concerns reportedly remain a roadblock to mass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’ purchase decisions, and whether such considerations are held only by certain consumer groups but not others. In order to unpack the decision-making process of smart home device adoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our analysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that consumers can be segmented based on their considerations into three clusters: affordability-oriented, privacy-oriented, and reliability-oriented. We present an in-depth quantification of consumer considerations on smart home device adoption along with desired privacy and security features consumers wish to use to protect their privacy in the smart home.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;While consumer adoption of smart home devices continues to grow, privacy concerns reportedly remain a roadblock to mass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’ purchase decisions, and whether such considerations are held only by certain consumer groups but not others. In order to unpack the decision-making process of smart home device adoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our analysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that consumers can be segmented based on their considerations into three clusters: affordability-oriented, privacy-oriented, and reliability-oriented. We present an in-depth quantification of consumer considerations on smart home device adoption along with desired privacy and security features consumers wish to use to protect their privacy in the smart home.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2020-smarthomeprivacy\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2020-smarthomeprivacy\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Natã Barbosa\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Natã Barbosa\&quot;},\&quot;description\&quot;:\&quot;While consumer adoption of smart home devices continues to grow, privacy concerns reportedly remain a roadblock to mass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’ purchase decisions, and whether such considerations are held only by certain consumer groups but not others. In order to unpack the decision-making process of smart home device adoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our analysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that consumers can be segmented based on their considerations into three clusters: affordability-oriented, privacy-oriented, and reliability-oriented. We present an in-depth quantification of consumer considerations on smart home device adoption along with desired privacy and security features consumers wish to use to protect their privacy in the smart home.\&quot;,\&quot;headline\&quot;:\&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2020-smarthomeprivacy\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  While consumer adoption of smart home devices continues\nto grow, privacy concerns reportedly remain a roadblock to\nmass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’\npurchase decisions, and whether such considerations are held\nonly by certain consumer groups but not others. In order to\nunpack the decision-making process of smart home device\nadoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our\nanalysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that\nconsumers can be segmented based on their considerations\ninto three clusters: affordability-oriented, privacy-oriented,\nand reliability-oriented. We present an in-depth quantification\nof consumer considerations on smart home device adoption\nalong with desired privacy and security features consumers\nwish to use to protect their privacy in the smart home.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2019-talkit&quot;,&quot;url&quot;:&quot;/publications/2019-talkit&quot;,&quot;relative_path&quot;:&quot;_publications/2019-talkit.html&quot;,&quot;path&quot;:&quot;_publications/2019-talkit.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;title&quot;:&quot;Designing interactive 3D printed models with Teachers of the Visually Impaired&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300427&quot;,&quot;authors&quot;:[&quot;Lei Shi&quot;,&quot;Holly Lawson&quot;,&quot;Zhuohao Zhang&quot;,&quot;Shiri Azenkot&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300427&quot;,&quot;video&quot;:&quot;https://drive.google.com/file/d/1QuJlDkc0Eq9pVeyYWjHgX7hrk9ksX7Eb/preview&quot;,&quot;blog&quot;:&quot;https://equalentry.com/interactive-3d-printed-models-for-students-with-visual-impairments-accessibility-nyc-meetup-recap/&quot;,&quot;slug&quot;:&quot;2019-talkit&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2020-smarthomeprivacy&quot;,&quot;relative_path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;excerpt&quot;:&quot;While consumer adoption of smart home devices continues\nto grow, privacy concerns reportedly remain a roadblock to\nmass adoption. However, it is unclear exactly how the interplay between privacy and other factors affect consumers’\npurchase decisions, and whether such considerations are held\nonly by certain consumer groups but not others. In order to\nunpack the decision-making process of smart home device\nadoption, we conducted a mixed-method analysis using online survey data collected from 631 US participants. Our\nanalysis uncovered motivators and blockers of purchase decisions, along with their relative importance. We found that\nconsumers can be segmented based on their considerations\ninto three clusters: affordability-oriented, privacy-oriented,\nand reliability-oriented. We present an in-depth quantification\nof consumer considerations on smart home device adoption\nalong with desired privacy and security features consumers\nwish to use to protect their privacy in the smart home.&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-collabally&quot;,&quot;url&quot;:&quot;/publications/2021-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3441852.3476562&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;ASSETS Demo&quot;,&quot;venue_tags&quot;:[&quot;ASSETS&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3441852.3476562&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=hYvAZofLSjU&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2020-smarthomeprivacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;title&quot;:&quot;Do Privacy and Security Matter to Everyone? Quantifying and Clustering User-Centric Considerations About Smart Home Device Adoption&quot;,&quot;authors&quot;:[&quot;Natã Barbosa&quot;,&quot;Zhuohao Zhang&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2020-barbosa.pdf&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=ORc2MwR1uQY&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2020-paper162-slides-barbosa.pdf&quot;,&quot;slug&quot;:&quot;2020-smarthomeprivacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2021-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;excerpt&quot;:&quot;Collaborative document editing tools are widely used in both professional and academic workplaces. While these tools provide some\naccessibility features, it is still challenging for blind users to gain\ncollaboration awareness that sighted people can easily obtain using\nvisual cues (e.g., who edited or commented where and what in the\ndocument). To address this gap, we present CollabAlly, a browser\nextension that makes extractable collaborative and contextual information in document editing accessible for blind users. With\nCollabAlly, blind users can easily access collaborators’ information,\ntrack real-time or asynchronous content and comment changes,\nand navigate through these elements. In order to convey this complex information through audio, CollabAlly uses voice fonts and\nspatial audio to enhance users’ collaboration awareness in shared\ndocuments. Through a series of pilot studies with a coauthor who is\nblind, CollabAlly’s design was refined to include more information\nand to be more compatible with existing screen readers&quot;,&quot;next&quot;:{&quot;content&quot;:&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang\&quot;},\&quot;description\&quot;:\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot;,\&quot;headline\&quot;:\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-webally&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-collabally&quot;,&quot;url&quot;:&quot;/publications/2021-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3441852.3476562&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;ASSETS Demo&quot;,&quot;venue_tags&quot;:[&quot;ASSETS&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3441852.3476562&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=hYvAZofLSjU&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2021-webally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;excerpt&quot;:&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-collabally&quot;,&quot;url&quot;:&quot;/publications/2022-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2022-collabally.html&quot;,&quot;path&quot;:&quot;_publications/2022-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3491102.3517635&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;awards&quot;:[&quot;Best Paper Honorable Mentions&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;pdf&quot;:&quot;https://guoanhong.com/papers/CHI22-CollabAlly.pdf&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=s5bcZQlmbzE&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=WK_8cPMVtuA&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2022-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang&quot;,&quot;Zhilin Zhang&quot;,&quot;Haolin Yuan&quot;,&quot;Natã Barbosa&quot;,&quot;Sauvik Das&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2021-zhang-zhuohao.pdf&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2021_slides_zhang_zhuohao.pdf&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=RMqrhzlc2us&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-webally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3441852.3476562&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;ASSETS Demo&quot;,&quot;venue_tags&quot;:[&quot;ASSETS&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3441852.3476562&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=hYvAZofLSjU&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2021-webally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;excerpt&quot;:&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.&quot;,&quot;next&quot;:{&quot;content&quot;:&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations. &quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;CollabAlly: Accessible Collaboration Awareness in Document Editing | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2022-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2022-collabally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang*\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang*\&quot;},\&quot;description\&quot;:\&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations.\&quot;,\&quot;headline\&quot;:\&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2022-collabally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations. \n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2022-collabally&quot;,&quot;previous&quot;:{&quot;content&quot;:&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#472f7e\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments | Zhuohao (Jerry) Zhang&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhuohao Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Zhuohao (Jerry) Zhang\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot; /&gt;\n&lt;meta name=\&quot;twitter:site\&quot; content=\&quot;@ZhuohaoZhang\&quot; /&gt;\n&lt;meta name=\&quot;twitter:creator\&quot; content=\&quot;@Zhuohao Zhang\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhuohao Zhang\&quot;},\&quot;description\&quot;:\&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs’ existing workflows.\&quot;,\&quot;headline\&quot;:\&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments\&quot;,\&quot;url\&quot;:\&quot;https://www.zhuohaozhang.com/publications/2021-webally\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Zhuohao (Jerry) Zhang\&quot; href=\&quot;https://www.zhuohaozhang.com/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n\n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Zhuohao (Jerry) Zhang&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/publications/\&quot; aria-current=\&quot;page\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/ZhuohaoZhang-CV.pdf\&quot; target=\&quot;_blank\&quot;&gt;Download CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;!-- &lt;a href=\&quot;https://twitter.com/ZhuohaoZhang\&quot;&gt;&lt;i class=\&quot;fab fa-twitter\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; Twitter&lt;/a&gt;\n      &lt;a href=\&quot;https://www.linkedin.com/in/zhuohao-zhang-08656510a/\&quot;&gt;&lt;i class=\&quot;fab fa-linkedin\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; LinkedIn&lt;/a&gt;\n      &lt;a href=\&quot;https://github.com/zhuohaouw\&quot;&gt;&lt;i class=\&quot;fab fa-github\&quot; aria-hidden=\&quot;true\&quot;&gt;&lt;/i&gt; GitHub&lt;/a&gt; --&gt;\n\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Website template forked from &lt;br/&gt;&lt;a href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;GitHub&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;Hi, I am always interested in research collaborations, regarding HCI, a11y, and AI. Feel free to shoot me an email! :)&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, &#39;&#39;);\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = &#39;&lt;a href=\&quot;#&#39; + name + &#39;\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39; + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-webally&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-collabally&quot;,&quot;url&quot;:&quot;/publications/2021-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;path&quot;:&quot;_publications/2021-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3441852.3476562&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;ASSETS Demo&quot;,&quot;venue_tags&quot;:[&quot;ASSETS&quot;],&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3441852.3476562&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=hYvAZofLSjU&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2021-webally&quot;,&quot;relative_path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;excerpt&quot;:&quot;Task-based visual CAPTCHAs are a significant accessibility hurdle for people with visual impairments (PVIs). What if PVIs could transfer task-based visual CAPTCHAs to a helper to solve? How might PVIs want such a system configured in terms of from whom they would solicit help and how they would compensate this help? To answer these questions, we implemented and evaluated a proof-of-concept assistive transfer system — WebAlly — that makes task-based CAPTCHAs transferable by allowing PVIs to source just-in-time, remote control help from a trusted contact. In an exploratory, role-play study with 10 pairs of participants — a PVI and a friend or a family member — we asked participants to use WebAlly in four different configurations that varied in source of help (friend vs. stranger) and compensation (paid vs. volunteer). We found that PVIs liked having WebAlly as an additional option for solving visual CAPTCHAs, when other options that preserve their independence fail. In addition, many PVIs and their friends felt that using the system would bring their relationship closer. We discuss design implications for transferable CAPTCHAs and assistive transfer systems more broadly, e.g., the importance of complementing rather than replacing PVIs&#39; existing workflows.&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-collabally&quot;,&quot;url&quot;:&quot;/publications/2022-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2022-collabally.html&quot;,&quot;path&quot;:&quot;_publications/2022-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3491102.3517635&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;awards&quot;:[&quot;Best Paper Honorable Mentions&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;pdf&quot;:&quot;https://guoanhong.com/papers/CHI22-CollabAlly.pdf&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=s5bcZQlmbzE&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=WK_8cPMVtuA&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2022-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang&quot;,&quot;Zhilin Zhang&quot;,&quot;Haolin Yuan&quot;,&quot;Natã Barbosa&quot;,&quot;Sauvik Das&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2021-zhang-zhuohao.pdf&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2021_slides_zhang_zhuohao.pdf&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=RMqrhzlc2us&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-webally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;url&quot;:&quot;/publications/2022-collabally&quot;,&quot;relative_path&quot;:&quot;_publications/2022-collabally.html&quot;,&quot;excerpt&quot;:&quot;Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified concrete challenges of blind people using collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations. &quot;,&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2022-collabally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;title&quot;:&quot;CollabAlly: Accessible Collaboration Awareness in Document Editing&quot;,&quot;doi&quot;:&quot;10.1145/3491102.3517635&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang*&quot;,&quot;Cheuk Yin Phipson Lee*&quot;,&quot;Jaylin Herskovitz&quot;,&quot;JooYoung Seo&quot;,&quot;Anhong Guo&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;awards&quot;:[&quot;Best Paper Honorable Mentions&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;pdf&quot;:&quot;https://guoanhong.com/papers/CHI22-CollabAlly.pdf&quot;,&quot;video&quot;:&quot;https://www.youtube.com/watch?v=s5bcZQlmbzE&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=WK_8cPMVtuA&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2022-collabally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]},&quot;path&quot;:&quot;_publications/2021-webally.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;title&quot;:&quot;WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments&quot;,&quot;authors&quot;:[&quot;Zhuohao Zhang&quot;,&quot;Zhilin Zhang&quot;,&quot;Haolin Yuan&quot;,&quot;Natã Barbosa&quot;,&quot;Sauvik Das&quot;,&quot;Yang Wang&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;venue&quot;:&quot;SOUPS&quot;,&quot;venue_tags&quot;:[&quot;SOUPS&quot;],&quot;pdf&quot;:&quot;https://www.usenix.org/system/files/soups2021-zhang-zhuohao.pdf&quot;,&quot;slide&quot;:&quot;https://www.usenix.org/system/files/soups2021_slides_zhang_zhuohao.pdf&quot;,&quot;recording&quot;:&quot;https://www.youtube.com/watch?v=RMqrhzlc2us&quot;,&quot;highlight&quot;:true,&quot;slug&quot;:&quot;2021-webally&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;tags&quot;:[]}">
  <h3 id="/publications/2021-webally">WebAlly: Making Visual Task-based CAPTCHAs Transferable for People with Visual Impairments</h3>
  <div class="authors">
    Zhuohao Zhang, 
    Zhilin Zhang, 
    Haolin Yuan, 
    Natã Barbosa, 
    Sauvik Das, 
    Yang Wang
    
  </div>
  
    <div class="venue">
      
      SOUPS<!--
      
      -->
      2021
    </div>
  

  

  

  
    <div class="extra-links">
    
      <a href="https://www.usenix.org/system/files/soups2021-zhang-zhuohao.pdf" target="_blank">
        <i class="far fa-file-pdf" aria-hidden="true"></i> PDF
      </a>
    
    
    
    
    
    
      <a href="https://www.youtube.com/watch?v=RMqrhzlc2us" target="_blank">
        <i class="fas fa-video" aria-hidden="true"></i> Recording
      </a>
    
    
    
    
    
    
    
      <a href="https://www.usenix.org/system/files/soups2021_slides_zhang_zhuohao.pdf" target="_blank">
        <i class="fas fa-file-powerpoint" aria-hidden="true"></i> Slides
      </a>
    
    
    </div>
  
</div>

          

        </section>

      </article>

      




      <!--
        - #PORTFOLIO
      -->

      <article class="portfolio" data-page="portfolio">

        <header>
          <h2 class="h2 article-title">Portfolio</h2>
        </header>

        <section class="projects">

          <ul class="filter-list">

            <li class="filter-item">
              <button class="active" data-filter-btn>All</button>
            </li>

            <li class="filter-item">
              <button data-filter-btn>Web design</button>
            </li>

            <li class="filter-item">
              <button data-filter-btn>Applications</button>
            </li>

            <li class="filter-item">
              <button data-filter-btn>Web development</button>
            </li>

          </ul>

          <div class="filter-select-box">

            <button class="filter-select" data-select>

              <div class="select-value" data-selecct-value>Select category</div>

              <div class="select-icon">
                <ion-icon name="chevron-down"></ion-icon>
              </div>

            </button>

            <ul class="select-list">

              <li class="select-item">
                <button data-select-item>All</button>
              </li>

              <li class="select-item">
                <button data-select-item>Web design</button>
              </li>

              <li class="select-item">
                <button data-select-item>Applications</button>
              </li>

              <li class="select-item">
                <button data-select-item>Web development</button>
              </li>

            </ul>

          </div>

          <ul class="project-list">

            <li class="project-item  active" data-filter-item data-category="web development">
              <a href="#">

                <figure class="project-img">
                  <div class="project-item-icon-box">
                    <ion-icon name="eye-outline"></ion-icon>
                  </div>

                  <img src="./assets/images/project-1.jpg" alt="finance" loading="lazy">
                </figure>

                <h3 class="project-title">Finance</h3>

                <p class="project-category">Web development</p>

              </a>
            </li>

            <li class="project-item  active" data-filter-item data-category="web development">
              <a href="#">

                <figure class="project-img">
                  <div class="project-item-icon-box">
                    <ion-icon name="eye-outline"></ion-icon>
                  </div>

                  <img src="./assets/images/project-2.png" alt="orizon" loading="lazy">
                </figure>

                <h3 class="project-title">Orizon</h3>

                <p class="project-category">Web development</p>

              </a>
            </li>

            <li class="project-item  active" data-filter-item data-category="web design">
              <a href="#">

                <figure class="project-img">
                  <div class="project-item-icon-box">
                    <ion-icon name="eye-outline"></ion-icon>
                  </div>

                  <img src="./assets/images/project-3.jpg" alt="fundo" loading="lazy">
                </figure>

                <h3 class="project-title">Fundo</h3>

                <p class="project-category">Web design</p>

              </a>
            </li>

            <li class="project-item  active" data-filter-item data-category="applications">
              <a href="#">

                <figure class="project-img">
                  <div class="project-item-icon-box">
                    <ion-icon name="eye-outline"></ion-icon>
                  </div>

                  <img src="./assets/images/project-4.png" alt="brawlhalla" loading="lazy">
                </figure>

                <h3 class="project-title">Brawlhalla</h3>

                <p class="project-category">Applications</p>

              </a>
            </li>

            <li class="project-item  active" data-filter-item data-category="web design">
              <a href="#">

                <figure class="project-img">
                  <div class="project-item-icon-box">
                    <ion-icon name="eye-outline"></ion-icon>
                  </div>

                  <img src="./assets/images/project-5.png" alt="dsm." loading="lazy">
                </figure>

                <h3 class="project-title">DSM.</h3>

                <p class="project-category">Web design</p>

              </a>
            </li>

            <li class="project-item  active" data-filter-item data-category="web design">
              <a href="#">

                <figure class="project-img">
                  <div class="project-item-icon-box">
                    <ion-icon name="eye-outline"></ion-icon>
                  </div>

                  <img src="./assets/images/project-6.png" alt="metaspark" loading="lazy">
                </figure>

                <h3 class="project-title">MetaSpark</h3>

                <p class="project-category">Web design</p>

              </a>
            </li>

            <li class="project-item  active" data-filter-item data-category="web development">
              <a href="#">

                <figure class="project-img">
                  <div class="project-item-icon-box">
                    <ion-icon name="eye-outline"></ion-icon>
                  </div>

                  <img src="./assets/images/project-7.png" alt="summary" loading="lazy">
                </figure>

                <h3 class="project-title">Summary</h3>

                <p class="project-category">Web development</p>

              </a>
            </li>

            <li class="project-item  active" data-filter-item data-category="applications">
              <a href="#">

                <figure class="project-img">
                  <div class="project-item-icon-box">
                    <ion-icon name="eye-outline"></ion-icon>
                  </div>

                  <img src="./assets/images/project-8.jpg" alt="task manager" loading="lazy">
                </figure>

                <h3 class="project-title">Task Manager</h3>

                <p class="project-category">Applications</p>

              </a>
            </li>

            <li class="project-item  active" data-filter-item data-category="web development">
              <a href="#">

                <figure class="project-img">
                  <div class="project-item-icon-box">
                    <ion-icon name="eye-outline"></ion-icon>
                  </div>

                  <img src="./assets/images/project-9.png" alt="arrival" loading="lazy">
                </figure>

                <h3 class="project-title">Arrival</h3>

                <p class="project-category">Web development</p>

              </a>
            </li>

          </ul>

        </section>

      </article>





      <!--
        - #BLOG
      -->

      <article class="blog" data-page="blog">

        <header>
          <h2 class="h2 article-title">Blog</h2>
        </header>

        <section class="blog-posts">

          <ul class="blog-posts-list">

            <li class="blog-post-item">
              <a href="#">

                <figure class="blog-banner-box">
                  <img src="./assets/images/blog-1.jpg" alt="Design conferences in 2022" loading="lazy">
                </figure>

                <div class="blog-content">

                  <div class="blog-meta">
                    <p class="blog-category">Design</p>

                    <span class="dot"></span>

                    <time datetime="2022-02-23">Fab 23, 2022</time>
                  </div>

                  <h3 class="h3 blog-item-title">Design conferences in 2022</h3>

                  <p class="blog-text">
                    Veritatis et quasi architecto beatae vitae dicta sunt, explicabo.
                  </p>

                </div>

              </a>
            </li>

            <li class="blog-post-item">
              <a href="#">

                <figure class="blog-banner-box">
                  <img src="./assets/images/blog-2.jpg" alt="Best fonts every designer" loading="lazy">
                </figure>

                <div class="blog-content">

                  <div class="blog-meta">
                    <p class="blog-category">Design</p>

                    <span class="dot"></span>

                    <time datetime="2022-02-23">Fab 23, 2022</time>
                  </div>

                  <h3 class="h3 blog-item-title">Best fonts every designer</h3>

                  <p class="blog-text">
                    Sed ut perspiciatis, nam libero tempore, cum soluta nobis est eligendi.
                  </p>

                </div>

              </a>
            </li>

            <li class="blog-post-item">
              <a href="#">

                <figure class="blog-banner-box">
                  <img src="./assets/images/blog-3.jpg" alt="Design digest #80" loading="lazy">
                </figure>

                <div class="blog-content">

                  <div class="blog-meta">
                    <p class="blog-category">Design</p>

                    <span class="dot"></span>

                    <time datetime="2022-02-23">Fab 23, 2022</time>
                  </div>

                  <h3 class="h3 blog-item-title">Design digest #80</h3>

                  <p class="blog-text">
                    Excepteur sint occaecat cupidatat no proident, quis nostrum exercitationem ullam corporis suscipit.
                  </p>

                </div>

              </a>
            </li>

            <li class="blog-post-item">
              <a href="#">

                <figure class="blog-banner-box">
                  <img src="./assets/images/blog-4.jpg" alt="UI interactions of the week" loading="lazy">
                </figure>

                <div class="blog-content">

                  <div class="blog-meta">
                    <p class="blog-category">Design</p>

                    <span class="dot"></span>

                    <time datetime="2022-02-23">Fab 23, 2022</time>
                  </div>

                  <h3 class="h3 blog-item-title">UI interactions of the week</h3>

                  <p class="blog-text">
                    Enim ad minim veniam, consectetur adipiscing elit, quis nostrud exercitation ullamco laboris nisi.
                  </p>

                </div>

              </a>
            </li>

            <li class="blog-post-item">
              <a href="#">

                <figure class="blog-banner-box">
                  <img src="./assets/images/blog-5.jpg" alt="The forgotten art of spacing" loading="lazy">
                </figure>

                <div class="blog-content">

                  <div class="blog-meta">
                    <p class="blog-category">Design</p>

                    <span class="dot"></span>

                    <time datetime="2022-02-23">Fab 23, 2022</time>
                  </div>

                  <h3 class="h3 blog-item-title">The forgotten art of spacing</h3>

                  <p class="blog-text">
                    Maxime placeat, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
                  </p>

                </div>

              </a>
            </li>

            <li class="blog-post-item">
              <a href="#">

                <figure class="blog-banner-box">
                  <img src="./assets/images/blog-6.jpg" alt="Design digest #79" loading="lazy">
                </figure>

                <div class="blog-content">

                  <div class="blog-meta">
                    <p class="blog-category">Design</p>

                    <span class="dot"></span>

                    <time datetime="2022-02-23">Fab 23, 2022</time>
                  </div>

                  <h3 class="h3 blog-item-title">Design digest #79</h3>

                  <p class="blog-text">
                    Optio cumque nihil impedit uo minus quod maxime placeat, velit esse cillum.
                  </p>

                </div>

              </a>
            </li>

          </ul>

        </section>

      </article>





      <!--
        - #CONTACT
      -->

      <article class="contact" data-page="contact">

        <header>
          <h2 class="h2 article-title">Contact</h2>
        </header>

        <section class="mapbox" data-mapbox>
          <figure>
            <iframe
              src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d199666.5651251294!2d-121.58334177520186!3d38.56165006739519!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x809ac672b28397f9%3A0x921f6aaa74197fdb!2sSacramento%2C%20CA%2C%20USA!5e0!3m2!1sen!2sbd!4v1647608789441!5m2!1sen!2sbd"
              width="400" height="300" loading="lazy"></iframe>
          </figure>
        </section>

        <section class="contact-form">

          <h3 class="h3 form-title">Contact Form</h3>

          <form action="#" class="form" data-form>

            <div class="input-wrapper">
              <input type="text" name="fullname" class="form-input" placeholder="Full name" required data-form-input>

              <input type="email" name="email" class="form-input" placeholder="Email address" required data-form-input>
            </div>

            <textarea name="message" class="form-input" placeholder="Your Message" required data-form-input></textarea>

            <button class="form-btn" type="submit" disabled data-form-btn>
              <ion-icon name="paper-plane"></ion-icon>
              <span>Send Message</span>
            </button>

          </form>

        </section>

      </article>

    </div>

  </main>






  <!--
    - custom js link
  -->
  <script src="./assets/js/script.js"></script>

  <!--
    - ionicon link
  -->
  <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
  <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>

</body>

</html>
